{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook closely follows Chapter 3 in:\n",
    "> &copy; Manohar SwamynathanÂ 2019\n",
    ">\n",
    "> M. SwamynathanMastering Machine Learning with Python in Six Steps\n",
    ">\n",
    "> https://doi.org/10.1007/978-1-4842-4947-5_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning-Classification\n",
    "==================================\n",
    "> In ML, classification deals with identifying the probability a new object is a member of a class or set. The classifiers are the algorighms that map the input data (also called features) to categories.\n",
    "\n",
    "\n",
    "Logistic Regression\n",
    "-------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "plt.style.use(['seaborn-talk', 'seaborn-darkgrid'])\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"../data/Grade_Set_1_Classification.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = df.Hours_Studied[:, np.newaxis]\n",
    "y = df.Result\n",
    "\n",
    "# Create linear regression object\n",
    "lr = lm.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "lr.fit(x, y)\n",
    "y_p = lr.predict(x)\n",
    "\n",
    "# Plot the fit line\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_p, lw=3)\n",
    "plt.title(\"Hours Studied vs Result\")\n",
    "plt.xlabel(\"Hours_Studied\")\n",
    "plt.ylabel(\"Result\")\n",
    "\n",
    "# Add predict value to the dataframe\n",
    "df['Result_Pred'] = y_p\n",
    "\n",
    "# Using built-in function\n",
    "print(\"R Squared:\", r2_score(df.Result, df.Result_Pred))\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(df.Result, df.Result_Pred))\n",
    "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(df.Result, df.Result_Pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "> Introduce a sigmoid or logit function (which takes an S shape) to the regression equation.\n",
    "\n",
    "> The fundamental idea here is that the hypothesis will use the linear approximation, then map with a logistic function for binary prediction.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{odds ratio of pass vs fail} = \\text{probability} (\\gamma = 1) / \\left[1 - \\text{probability} (\\gamma = 1) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "> A logit is the log base e of the odds:\n",
    "\\begin{equation}\n",
    "\\log (p / (1 - p)) = mx + c\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sigmoid function\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "plt.plot(x, y, label=\"logit\")\n",
    "plt.plot(x[[0, -1]], y[[0, -1]], '--')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Manually assign intercept\n",
    "df['intercept'] = 1\n",
    "independent_variables = ['Hours_Studied', 'intercept']\n",
    "\n",
    "x = df[independent_variables]\n",
    "y = df['Result']\n",
    "\n",
    "model = LogisticRegression()\n",
    "model = model.fit(x, y)\n",
    "\n",
    "# Check the accuracy on the training set\n",
    "model.score(x, y)\n",
    "\n",
    "print(\"Predicted probability:\\n\", model.predict_proba(x)[:, 1])\n",
    "\n",
    "print(\"Predicted class:\\n\", model.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.Hours_Studied, y)\n",
    "plt.yticks([0.0, 0.5, 1.0])\n",
    "plt.plot(df.Hours_Studied, model.predict_proba(x)[:, 1], linewidth=3)\n",
    "plt.title(\"Hours Studied vs Result\")\n",
    "plt.xlabel(\"Hours_Studied\")\n",
    "plt.ylabel(\"Result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating a Classification Model Performance\n",
    "---------------------------------------------\n",
    "> - True Negative (TN): Actual FALSE that was predicted as FALSE\n",
    "> - False Positive (FP): Actual FALSE that was predicted as TRUE (Type I error)\n",
    "> - False Negative (FN): Actual TRUE that was predicted as FALSE (Type II error)\n",
    "> - True Positive (TP): Actual TRUE that was predicted as TRUE\n",
    "\n",
    "Classification Performance Matrices\n",
    "\n",
    "| Metric | Description | Formula |\n",
    "| ------ | ----------- | ------- |\n",
    "| Accuracy | What % of predictions was correct? | (TP + TN) / (TP + TN + FP + FN) |\n",
    "| Misclasification Rate | What % of prediction is wrong? | (FP + FN) / (TP + TN + FP + FN) |\n",
    "| True Positive Rate OR Sensitivity or Recall (completeness) | What % of positive cases did the model catch? | TP / (FN + TP) |\n",
    "| False Positive Rate | What % of No was predicted as Yes? | FP / (FP + TN) |\n",
    "| Specificity | What % of No was predicted as No? | TN / (TN + FP) |\n",
    "| Precision (exactness) | What % of positive predictions was correct? | TP / (TP + FP) |\n",
    "| F1 score | Weighted average of precision and recall | 2 * (precision * recall) / (precision + recall) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_p = model.predict(x)\n",
    "\n",
    "# Generate evaluation metrics\n",
    "print(\"Accuracy:\\n\", metrics.accuracy_score(y, y_p))\n",
    "print(\"AUC:\\n\", metrics.roc_auc_score(y, model.predict_proba(x)[:, 1]))\n",
    "\n",
    "print(\"Confusion matrix:\\n\", metrics.confusion_matrix(y, model.predict(x)))\n",
    "print(\"Classification report:\\n\", metrics.classification_report(y, y_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Curve\n",
    "---------\n",
    "> An ROC (receiver operating characteristic) curve is one more important metric, a most commonly used way to visualize the performance of a binary claaier; and AUC is believed to be one of the best ways to summarize performance in a single number. AUC indicated that the probability of a randomly selected positive example will be scored higher by the classifier than a randomly selected negative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determin the false positive and true positive rated\n",
    "fpr, tpr, _ = metrics.roc_curve(y, model.predict_proba(x)[:, 1])\n",
    "\n",
    "# Calculate the AUC\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of a ROC curve for a specific class\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Line\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate a logistic regression model with different c values, and fit with x and y\n",
    "models = []\n",
    "Cs = [1, 10, 100, 1000]\n",
    "for C in Cs:\n",
    "    model = LogisticRegression(C=C)\n",
    "    model = model.fit(x, y)\n",
    "    models.append(model)\n",
    "    y_p = model.predict(x)\n",
    "\n",
    "    # Check the accuracy on the training set\n",
    "    print(f\"{C = :4d},\\tAccuracy: {metrics.accuracy_score(y, y_p)}\")\n",
    "\n",
    "# Plot fit line\n",
    "plt.scatter(df.Hours_Studied, y, label=\"Result\")\n",
    "plt.yticks([0.0, 0.5, 1.0])\n",
    "\n",
    "for i in range(len(Cs)):\n",
    "    C = Cs[i]\n",
    "    model = models[i]\n",
    "    plt.plot(df.Hours_Studied, model.predict_proba(x)[:, 1], lw=2, label=f\"{C = }\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Hours Studied vs Result\")\n",
    "plt.xlabel(\"Hours_Studied\")\n",
    "plt.ylabel(\"Result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the case of regression problems, the *cost function* $J$ to learn the weights can be defined as the sum of squared errors (SSE) between actual vs. predicted value.\n",
    "\\begin{equation}\n",
    "J(w) = \\frac{1}{2} \\sum_i \\left( y^i - \\hat{y}^i \\right),\n",
    "\\end{equation}\n",
    "> where $y^i$ is the $i$-th actual value, and $\\hat{y}^i$ is the $i$-th predicted value.\n",
    "\n",
    "> The stochastic gradient descent algorith to update weight ($w$), for every weight $j$ of every training sample $i$, can be given as (repeat until converging)\n",
    "\\begin{equation}\n",
    "W_j := W_j + \\alpha \\sum_{i=1}^m \\left( y^i - \\hat{y}^i \\right) x_j^i.\n",
    "\\end{equation}\n",
    "> Alpha ($\\alpha$) is the learning rate, and choosing a smaller value for the same will ensure that the algorithm does not miss global cost minimum.\n",
    "\n",
    "> The default solver parameter for logistic regression in Scikit-learn is `liblinear`, which works fine for the smaller dataset. For a large dataset witha large number of independent variables, `sag` (stochastic avearage gradient descent) is the recommended solver to fit the optimal slope faster.\n",
    "\n",
    "\n",
    "Regularization\n",
    "--------------\n",
    "> With an increase in the number of variables, the probability of overfitting also increases. LASSO (L1) and Ridge (L2) can be applied for logistic regression as well, to avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../data/LR_NonLinear.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pos = data['class'] == 1\n",
    "neg = data['class'] == 0\n",
    "x1 = data['x1']\n",
    "x2 = data['x2']\n",
    "\n",
    "# Function to draw scatter plot between two variables\n",
    "def draw_plot(x1, x2, pos, neg, C):\n",
    "    plt.figure()\n",
    "    plt.scatter(x1[pos], x2[pos], marker=\"s\", label=\"pos\")\n",
    "    plt.scatter(x1[neg], x2[neg], marker=\"o\", label=\"neg\")\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.axes().set_aspect('equal', 'datalim')\n",
    "    plt.legend()\n",
    "    plt.title(f\"Fitting with {C = }\")\n",
    "\n",
    "\n",
    "# Create higher order polynomial for independent variables\n",
    "order_no = 6\n",
    "\n",
    "# Map the variable 1 & 2 to its higher order polynomial\n",
    "def map_features(variable_1, variable_2, order=order_no):\n",
    "    assert order >= 1\n",
    "    def iter():\n",
    "        for i in range(1, order + 1):\n",
    "            for j in range(i + 1):\n",
    "                yield np.power(variable_1, i - j) * np.power(variable_2, j)\n",
    "    return np.vstack(iter())\n",
    "\n",
    "out = map_features(x1, x2, order=order_no)\n",
    "X = out.T\n",
    "y = data['class']\n",
    "\n",
    "# Split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Function to draw classifier line\n",
    "def draw_boundary(classifier):\n",
    "    dim = np.linspace(-0.8, 1.1, 100)\n",
    "    dx, dy = np.meshgrid(dim, dim)\n",
    "    v = map_features(dx.flatten(), dy.flatten(), order=order_no)\n",
    "    z = (np.dot(classifier.coef_, v) + classifier.intercept_).reshape(100, 100)\n",
    "    plt.contour(dx, dy, z, levels=[0], colors=['purple'])\n",
    "    plt.legend()\n",
    "\n",
    "# Fit with C = 0.01\n",
    "Cs = [0.01, 1, 10000]\n",
    "for C in Cs:\n",
    "    clf = LogisticRegression(C=C).fit(X_train, y_train)\n",
    "    print(f\"Train Accuracy for {C = }:\\t{clf.score(X_train, y_train)}\")\n",
    "    print(f\"Test  Accuracy for {C = }:\\t{clf.score(X_test, y_test)}\")\n",
    "\n",
    "    draw_plot(x1, x2, pos, neg, C)\n",
    "    draw_boundary(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that with higher order regularization, value overfitting occurs. The same can be determined by looking at the accuracy between train and test datasets(i.e., accuracy drops significantly in the test data set).\n",
    "\n",
    "\n",
    "Multiclass Logistic Regression\n",
    "------------------------------\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(\"Class labels:\", np.unique(y))\n",
    "print(f\"{X = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X)\n",
    "X = sc.transform(X)\n",
    "print(f\"{X = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "Split data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Logistic Regression Model and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l1', C=10, random_state=0, solver=\"liblinear\")\n",
    "lr.fit(X_train, y_train)\n",
    "y_ptr = lr.predict(X_train)\n",
    "y_pte = lr.predict(X_test)\n",
    "\n",
    "# Generate evaluation metrics\n",
    "print(\"Train - Accuracy:\\n\", metrics.accuracy_score(y_train, y_ptr))\n",
    "print(\"Train - Confusion matrix:\\n\", metrics.confusion_matrix(y_train, y_ptr))\n",
    "print(\"Train - Classification report:\\n\", metrics.classification_report(y_train, y_ptr))\n",
    "\n",
    "print(\"Test  - Accuracy:\\n\", metrics.accuracy_score(y_test, y_pte))\n",
    "print(\"Test  - Confusion matrix:\\n\", metrics.confusion_matrix(y_test, y_pte))\n",
    "print(\"Test  - Classification report:\\n\", metrics.classification_report(y_test, y_pte))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generalized Linear Models\n",
    "Different GLM Distribution Family\n",
    "\n",
    "| Family | Description |\n",
    "| ------ | ----------- |\n",
    "| Binomial | Target variable is binary response |\n",
    "| Poisson | Target variable is a count of occurrence |\n",
    "| Gaussian | Target variable is a continuous number |\n",
    "| Gamma | This distribution arises when the waiting times between Poisson distribution events are relevant (i.e., a number of events occurred between two time periods) |\n",
    "| Inverse Gaussian | The tails of the distribution decrease slower than normal distribution (i.e., there is an inverse relationship between the time required to cover a unit distance and distance covered in unit time) |\n",
    "| Negative Binomial | Target variable denotes the number of successes in a sequence before a random failure |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "df = pd.read_csv(\"../data/Grade_Set_1.csv\")\n",
    "print(\"####### Linear Regression Model #######\")\n",
    "\n",
    "# Create a linear regression object\n",
    "lr = lm.LinearRegression()\n",
    "\n",
    "x = df.Hours_Studied[:, np.newaxis]\n",
    "y = df.Test_Grade.values\n",
    "\n",
    "# Train the model using the training sets\n",
    "lr.fit(x, y)\n",
    "\n",
    "print(\"Intercept:\", lr.intercept_)\n",
    "print(\"Coefficient:\", lr.coef_)\n",
    "\n",
    "print(\"\\n####### Generalized Linear Model #######\")\n",
    "\n",
    "# In able to run GLM, we'll have to add the intercept constant to x variable\n",
    "x = sm.add_constant(x, prepend=False)\n",
    "\n",
    "# Instantiate a gaussian family model with the default link function\n",
    "model = sm.GLM(y, x, family=sm.families.Gaussian())\n",
    "model = model.fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(sm.add_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning-Process Flow\n",
    "================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First, you need to train and validate a supervised model by applying ML techniques to historical data. Then apply this model to the new data set to predict the future value.\n",
    "\n",
    "\n",
    "Decision Trees\n",
    "--------------\n",
    "> In 1986, J. R. Quinlan published [Introduction of Decision Trees](https://doi.org/10.1007/BF00116251) summarizing an approach to synthesizing decision trees using ML. A decision tree consists of three types of nodes:\n",
    "> - Root node\n",
    "> - Branch node\n",
    "> - Leaf node (class label)\n",
    "\n",
    "> The advantage of the decision tree is that there is no need for exclusive creation of *dummy variables*.\n",
    "\n",
    "\n",
    "### How the Tree Splits and Grows\n",
    "> - The base algorithm is known as a greedy algorithm, in which a tree is constructed in a top-down recursive divide-and-conquer manner.\n",
    "> - At the start, all the training examples are at the root.\n",
    "> - Input data is partitioned recursively based on selected attributes.\n",
    "> - Test attributes at each node are selected on the basis of a heuristic or statistical impurity measure example: Gini or information gain (entropy).\n",
    ">   - Gini = $1 - \\sum_i (p_i)^2$, where $p_i$ is the probability of each label.\n",
    ">   - Entropy = $-p \\log_2(p) - q \\log_2(q)$, where $p$ and $q$ represent the probability of success/failure respectively in a given node.\n",
    "\n",
    "\n",
    "### Conditions for Stopping Partitioning\n",
    "> - All samples for a given node belong to the same class.\n",
    "> - There are no remaining attributes for further partitioning--majority voting is employed for classifying the leaf.\n",
    "> - There are no samples left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X)\n",
    "X = sc.transform(X)\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_ptr = clf.predict(X_train)\n",
    "y_pte = clf.predict(X_test)\n",
    "\n",
    "# Generate evaluation metrics\n",
    "print(\"Train - Accuray:\\t\", metrics.accuracy_score(y_train, y_ptr))\n",
    "print(\"Train - Confusion matrix:\\t\", metrics.confusion_matrix(y_train, y_ptr))\n",
    "print(\"Train - Classification report:\\t\", metrics.classification_report(y_train, y_ptr))\n",
    "\n",
    "print(\"Test  - Accuracy:\\t\", metrics.accuracy_score(y_test, y_pte))\n",
    "print(\"Test  - Confusion matrix:\\t\", metrics.confusion_matrix(y_test, y_pte))\n",
    "print(\"Test  - Classification report:\\t\", metrics.classification_report(y_test, y_pte))\n",
    "\n",
    "tree.plot_tree(clf, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Key Parameters for Stopping Tree Growth\n",
    "> One of the key issues with a decision tree is that the tree can grow very large, ending up creating one leaf per observation.\n",
    "\n",
    "> *max_features*: Maximum features to be considered while deciding each split; default = `None`, which means all features will be considered.\n",
    "> *min_samples_split*: Split will not be allowed for nodes that do not meet this number.\n",
    "> *min_samples_leaf*: Leaf node will not be allowed for nodes less than the minimum samples.\n",
    "> *max_depth*: No further split will be allowed; default = `None`.\n",
    "\n",
    "\n",
    "Support Vector Machine\n",
    "----------------------\n",
    "> Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1963 proposed SVM. A key objective of SVM is to draw a hyperplane that separates the two classes optimally such that the margin is maximum between the hyperplane and the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X)\n",
    "X = sc.transform(X)\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "clf = SVC(kernel=\"linear\", C=1.0, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_ptr = clf.predict(X_train)\n",
    "y_pte = clf.predict(X_test)\n",
    "\n",
    "# Generate evaluation metrics\n",
    "print(\"Train - Accuracy:\\n\", metrics.accuracy_score(y_train, y_ptr))\n",
    "print(\"Train - Confusion matrix:\\n\",metrics.confusion_matrix(y_train, y_ptr))\n",
    "print(\"Train - classification report:\\n\", metrics.classification_report(y_train, y_ptr))\n",
    "\n",
    "print(\"Test  - Accuracy:\\n\", metrics.accuracy_score(y_test, y_pte))\n",
    "print(\"Test  - Confusion matrix:\\n\", metrics.confusion_matrix(y_test, y_pte))\n",
    "print(\"Test  - Classification report:\\n\", metrics.classification_report(y_test, y_pte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def plot_decision_regions(X, y, classifier):\n",
    "    h = .02 # step size in the mesh\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    # cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, h),\n",
    "                           np.arange(x2_min, x2_max, h))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, c=colors[idx],\n",
    "                    marker=markers[idx], label=cl)\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, n_redundant=0, weights=[.5, .5], random_state=0)\n",
    "\n",
    "# Build a simple logistic regression model\n",
    "clf = SVC(kernel=\"linear\", random_state=0)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Get the separating hyperplane\n",
    "w = clf.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(-1.5, 1.5)\n",
    "yy = a * xx - clf.intercept_[0] / w[1]\n",
    "\n",
    "# Plot the parallels to the separating hyperplane that pass through the support vectors\n",
    "b = clf.support_vectors_[0]\n",
    "yy_down = a * xx + (b[1] - a * b[0])\n",
    "b = clf.support_vectors_[-1]\n",
    "yy_up = a * xx + (b[1] - a * b[0])\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_decision_regions(X, y, classifier=clf)\n",
    "\n",
    "# Plot the line, the points, and the nearest vectors to the plane\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80)\n",
    "plt.plot(xx, yy_down, 'k--')\n",
    "plt.plot(xx, yy_up, 'k--')\n",
    "\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "$k$-Nearest Neighbors\n",
    "---------------------\n",
    "> The $k$-nearest neighbor classification (kNN) was developed from the need to perform discriminant analysis when reliable parametric estimates of probability densitieds are unknown or difficult to determine. Fix and Hodges in 1951 introduced a nonparametric method for pattern classification that has since become known as the $k$-nearest neighbor rule.\n",
    "\n",
    "> The key drawback of kNN is the complexity in searching the nearest neighbors for each sample.\n",
    "\n",
    "> - $k$ must **not** be a multiple of the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5, p=2, metric=\"minkowski\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Generate evaluation metrics\n",
    "print(\"Train - Accuracy:\\n\", metrics.accuracy_score(y_train, y_ptr))\n",
    "print(\"Train - Confusion matrix:\\n\",metrics.confusion_matrix(y_train, y_ptr))\n",
    "print(\"Train - classification report:\\n\", metrics.classification_report(y_train, y_ptr))\n",
    "\n",
    "print(\"Test  - Accuracy:\\n\", metrics.accuracy_score(y_test, y_pte))\n",
    "print(\"Test  - Confusion matrix:\\n\", metrics.confusion_matrix(y_test, y_pte))\n",
    "print(\"Test  - Classification report:\\n\", metrics.classification_report(y_test, y_pte))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time-Series Forecasting\n",
    "-----------------------\n",
    "> Time-series data having the mean and variance as constant is called a *stationary time series*.\n",
    "\n",
    "> Time series tend to have a linear relationship between lagged variables and this is called *autocorrelation*. Hence time-series historic data can be modeled to forecast future data points without the involvement of any other independent variables; these types of models are generally known as *time-series forecasting*.\n",
    "\n",
    "\n",
    "### Components of Time Series\n",
    "> A time series can be made up of three key components:\n",
    "> - *Trend*: A long-term increase or decrease is termed a trend.\n",
    "> - *Seasonality*: An effect of seasonal factors for a fixed or known period.\n",
    "> - *Cycle*: These are the longer ups and downs that are not of fixed or known period, caused by external factors.\n",
    "\n",
    "\n",
    "### Autoregressive Integrated Moving Average (ARIMA)\n",
    "> *Autoregressive model* (AM): a regression of the varaible against itself (i.e., the linear combination of past values of the variable is used to forecast the future value).\n",
    "\\begin{equation}\n",
    "y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\cdots + \\phi_n y_{t-n} + e_t,\n",
    "\\end{equation}\n",
    "> where $c$ is constant, $e_t$ is the random error, $y_{t-1}$ is the first-order correlation, and $y_{t-2}$ the second-order correlation between values two periods apart.\n",
    "\n",
    "> *Moving average* (MA): Instead of past values, past forecast errors are used to build a model.\n",
    "\\begin{equation}\n",
    "y_t = c + \\theta_1 y_{t-1} + \\theta_2 y_{t-2} + \\cdots + \\theta_n y_{t-n} + e_t.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/TS.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "ts = pd.Series(df[\"Sales\"].to_list(), index=pd.to_datetime(df.Month,\n",
    "                format=\"%Y-%m\"))\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decomposition = seasonal_decompose(ts)\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "fig, ax = plt.subplots(3, 1)\n",
    "\n",
    "ax[0].plot(ts, label=\"Original\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(trend, label=\"Trend\")\n",
    "ax[1].legend()\n",
    "ax[2].plot(seasonal, label=\"Seasonality\")\n",
    "ax[2].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Log transform\n",
    "ts_log = np.log(ts)\n",
    "ts_log.dropna(inplace=True)\n",
    "\n",
    "s_test = adfuller(ts_log, autolag=\"AIC\")\n",
    "\n",
    "print(\"Log transform stationary check p value:\", s_test[1])\n",
    "\n",
    "# Take first difference\n",
    "ts_log_diff = ts_log - ts_log.shift()\n",
    "ts_log_diff.dropna(inplace=True)\n",
    "\n",
    "plt.title(\"Trend removed plot with first order difference\")\n",
    "plt.plot(ts_log_diff)\n",
    "plt.ylabel(\"First order log diff\")\n",
    "\n",
    "s_test = adfuller(ts_log_diff, autolag=\"AIC\")\n",
    "\n",
    "print(\"First order difference stationary check p value:\", s_test[1])\n",
    "\n",
    "# Moving average smoothens the line\n",
    "moving_avg = ts_log.rolling(12).mean()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "ax1.set_title(\"First order difference\")\n",
    "ax1.tick_params(axis=\"x\", labelsize=7)\n",
    "ax1.tick_params(axis=\"y\", labelsize=7)\n",
    "ax1.plot(ts_log_diff)\n",
    "\n",
    "ax2.set_title(\"Log vs Moving Avg\")\n",
    "ax2.tick_params(axis=\"x\", labelsize=7)\n",
    "ax2.tick_params(axis=\"y\", labelsize=7)\n",
    "ax2.plot(ts_log)\n",
    "ax2.plot(moving_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 14))\n",
    "\n",
    "# ACF chart\n",
    "fig = sm.graphics.tsa.plot_acf(ts_log_diff.values.squeeze(), lags=20, ax=ax1)\n",
    "\n",
    "# Draw 95% confidence interval line\n",
    "ax1.axhline(y = -1.96 / np.sqrt(len(ts_log_diff)), linestyle='--', color='gray')\n",
    "ax1.axhline(y = +1.96 / np.sqrt(len(ts_log_diff)), linestyle='--', color='gray')\n",
    "ax1.set_xlabel(\"Lags\")\n",
    "\n",
    "# PACF chart\n",
    "fig = sm.graphics.tsa.plot_pacf(ts_log_diff, lags=20, ax=ax2)\n",
    "\n",
    "# Draw 95% confidence interval line\n",
    "ax2.axhline(y = -1.96 / np.sqrt(len(ts_log_diff)), linestyle='--', color='gray')\n",
    "ax2.axhline(y = +1.96 / np.sqrt(len(ts_log_diff)), linestyle='--', color='gray')\n",
    "ax2.set_xlabel(\"Lags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The PACF plot has a significant spike only at lag 1, meaning that all the higher order autocorrelations are effectively explained by the lag-1 and lag-2 autocorrelation. Ideal lag values are $p = 2$ and $q = 2$ (i.e., the lag value where the ACF/PACF chart crosses the upper confidence interval for the first time).\n",
    "\n",
    "\n",
    "### Build Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "pdq = (2, 0, 2)\n",
    "model = sm.tsa.ARIMA(ts_log, order=pdq)\n",
    "results_ARIMA = model.fit()\n",
    "\n",
    "ts_predict = results_ARIMA.predict()\n",
    "\n",
    "# Evaluate model\n",
    "print(\"AIC:\", results_ARIMA.aic)\n",
    "print(\"BIC:\", results_ARIMA.bic)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(ts_log.values, ts_predict.values))\n",
    "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(ts_log.values, ts_predict.values)))\n",
    "\n",
    "# Check autocorrelation\n",
    "print(\"Durbin-Watson statistics:\", sm.stats.durbin_watson(results_ARIMA.resid.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The usual practice is to build several models with different $P$ and $q$ and select the one with the smallest value of AIC, BIC, MAE, and RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdq = (3, 0, 2)\n",
    "model = sm.tsa.ARIMA(ts_log, order=pdq)\n",
    "results_ARIMA = model.fit()\n",
    "\n",
    "ts_predict = results_ARIMA.predict()\n",
    "plt.title(f\"ARIMA Prediction- order {pdq}\")\n",
    "plt.plot(ts_log, label=\"Actual\")\n",
    "plt.plot(ts_predict, '--', label=\"Predicted\")\n",
    "plt.xlabel(\"Year-Month\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.legend()\n",
    "\n",
    "print(\"AIC:\", results_ARIMA.aic)\n",
    "print(\"BIC:\", results_ARIMA.bic)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(ts_log.values, ts_predict.values))\n",
    "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(ts_log.values, ts_predict.values)))\n",
    "\n",
    "# Check autocorrelation\n",
    "print(\"Durbin-Watson statistic:\", sm.stats.durbin_watson(results_ARIMA.resid.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some unknown errors seem to appear in the following block. So I will comment it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pdq = (3, 1, 2)\n",
    "# model = sm.tsa.ARIMA(ts_log, order=pdq)\n",
    "# results_ARIMA = model.fit()\n",
    "\n",
    "# ts_predict = results_ARIMA.predict()\n",
    "\n",
    "# # Correction for difference\n",
    "# predictions_ARIMA_diff = pd.Series(ts_predict, copy=True)\n",
    "# predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
    "# predictions_ARIMA_log = pd.Series(ts_log.iloc[0], index=ts_log.index)\n",
    "# predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)\n",
    "\n",
    "# plt.title(f\"ARIMA Prediction - order {pdq}\")\n",
    "# plt.plot(ts_log, label=\"Actual\")\n",
    "# plt.plot(predictions_ARIMA_log, '--', label=\"Predicted\")\n",
    "# plt.xlabel(\"Year-Month\")\n",
    "# plt.ylabel(\"Sales\")\n",
    "# plt.legend()\n",
    "\n",
    "# print(\"AIC:\", results_ARIMA.aic)\n",
    "# print(\"BIC:\", results_ARIMA.bic)\n",
    "\n",
    "# print(\"Mean Absolute Error:\", mean_absolute_error(ts_log_diff.values, ts_predict.values))\n",
    "# print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(ts_log_diff.values, ts_predict.values)))\n",
    "\n",
    "# # Check autocorrelation\n",
    "# print(\"Durbin-Watson statistic:\", sm.stats.durbin_watson(results_ARIMA.resid.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Future Values\n",
    "Below values $(p=3, d=0, q=2)$ is giving the smaller number for evaluation metrics, so let's use this as a final model to predict the future values, for the year 1972."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Final model\n",
    "pdq = (3, 0, 2)\n",
    "model = sm.tsa.ARIMA(ts_log, order=pdq)\n",
    "results_ARIMA = model.fit()\n",
    "\n",
    "# Predict future values\n",
    "ts_predict = results_ARIMA.predict(\"1971-06-01\", \"1972-05-01\")\n",
    "\n",
    "plt.title(f\"ARIMA Future Value Prediction - order {pdq}\")\n",
    "plt.plot(ts_log, label=\"Actual\")\n",
    "plt.plot(ts_predict, '--', label=\"Predicted\")\n",
    "plt.xlabel(\"Year-Month\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised Learning Process Flow\n",
    "==================================\n",
    "\n",
    "\n",
    "Clustering\n",
    "----------\n",
    "> Clustering is an unsupevised learning problem. The key objective is to identify distinct groups (called *clusters*) based on some notion of similarity within a given dataset. Clustering analysis origins can be traced to the areas of anthropology and psychology in the 1930s. Most popularly used clustering techniques are $k$-means (divisive) and hierarchical (agglomerative).\n",
    "\n",
    "\n",
    "### $k$-means\n",
    "> The key objective of the $K$-means algorithm is to organize data into clusters such that there is a high intracluster similarity and low intercluster similarity. An item will only belong to one cluster, not several (i.e., it generates a specific number of disjoint, non-hierarchial clusters). $k$-means uses the strategy of divide and concur, and is a classic example for *expectation maximization* (EM) algorithms.\n",
    "\n",
    "> EM algorithms are made up of two steps:\n",
    ">\n",
    "> 1. known as the *expectation* (E), is to find the expected point associated with a cluster;\n",
    "> 2. known as *maximization* (M), is to improve the estimation of the cluster using knowledge from the first step.\n",
    ">\n",
    "> The two steps are processed repeatedly until convergence is reached.\n",
    "\n",
    "> Suppose we have $n$ data points that we need to cluster into $k$ (c1, c2, c3) groups.\n",
    ">\n",
    "> 1. $k$ centroids is randomly picked (only in the first iteration) and all the points that are nearest to each centroid point are assigned to that specific cluster. The centroid is the arithmetic mean or average position of all the points.\n",
    "> 2. The centroid point is recalculated using the average of the coordinates of all the points in that cluster. Then step one is repeated (assign the nearest point) until the clusters converge.\n",
    "\n",
    "> **Note**: $k$-means is designed for Euclidean distance only.\n",
    "\n",
    "\n",
    "### Limitations of $k$-means\n",
    "> - $k$-means clustering needs the number of clusters to be specified.\n",
    "> - $k$-means has problems when clusters are of different sizes, densities, and nonglobular shapes.\n",
    "> - The presence of an outlier can skew the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Convert to dataframe\n",
    "iris = pd.DataFrame(data = np.c_[iris['data'], iris['target']],\n",
    "                    columns = iris['feature_names'] + ['species'])\n",
    "\n",
    "# Remove spaces from column name\n",
    "iris.columns = iris.columns.str.replace(' ', '')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = iris.iloc[:, :3]\n",
    "y = iris.species\n",
    "sc = StandardScaler()\n",
    "sc.fit(X)\n",
    "X = sc.transform(X)\n",
    "\n",
    "# K Means Cluster\n",
    "model = KMeans(n_clusters=3, random_state=11)\n",
    "model.fit(X)\n",
    "print(model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare the above results with the actual species label to understand the accuracy of grouping similar records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "iris['species'] = iris.species.astype(np.int64)\n",
    "iris['pred_species'] = np.choose(model.labels_, [1, 0, 2]).astype(np.int64)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(iris.species, iris.pred_species))\n",
    "print(\"Classification report:\",metrics.classification_report(iris.species, iris.pred_species))\n",
    "\n",
    "# Set the size of the plot\n",
    "# plt.figure()\n",
    "\n",
    "# Create a colormap for red, green and blue\n",
    "cmap = ListedColormap(['r', 'g', 'b'])\n",
    "\n",
    "# Plot Sepal\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "ax[0, 0].scatter(iris['sepallength(cm)'], iris['sepalwidth(cm)'], c=cmap(iris.species), marker=\"o\", s=50)\n",
    "ax[0, 0].set_xlabel(\"sepallength(cm)\")\n",
    "ax[0, 0].set_ylabel(\"sepalwidth(cm)\")\n",
    "ax[0, 0].set_title(\"Sepal (Actual)\")\n",
    "\n",
    "ax[0, 1].scatter(iris['sepallength(cm)'], iris['sepalwidth(cm)'], c=cmap(iris.pred_species), marker=\"o\", s=50)\n",
    "ax[0, 1].set_xlabel(\"sepallength(cm)\")\n",
    "ax[0, 1].set_ylabel(\"sepalwidth(cm)\")\n",
    "ax[0, 1].set_title(\"Sepal (Predicted)\")\n",
    "\n",
    "ax[1, 0].scatter(iris['petallength(cm)'], iris['petalwidth(cm)'], c=cmap(iris.species), marker=\"o\", s=50)\n",
    "ax[1, 0].set_xlabel(\"petallength(cm)\")\n",
    "ax[1, 0].set_ylabel(\"petalwidth(cm)\")\n",
    "ax[1, 0].set_title(\"Petal (Actual)\")\n",
    "\n",
    "ax[1, 1].scatter(iris['petallength(cm)'], iris['petalwidth(cm)'], c=cmap(iris.pred_species), marker=\"o\", s=50)\n",
    "ax[1, 1].set_xlabel(\"petallength(cm)\")\n",
    "ax[1, 1].set_ylabel(\"petalwidth(cm)\")\n",
    "ax[1, 1].set_title(\"Petal (Predicted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Value of $k$\n",
    "> Two methods are commonly used to determine the value of $k$:\n",
    "> - Elbow method\n",
    "> - Average silhouette method\n",
    "\n",
    "\n",
    "### Elbow Method\n",
    "> Perform $k$-means clustering on the dataset for a range of value $k$ (for example 1 to 10) and calculate the SSE or percentage of variance explained for each $k$. Plot a line chart for cluster number vs. SSE, then look for an elbow shape on the line graph, which is the ideal number of cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist, pdist\n",
    "\n",
    "K = range(1, 10)\n",
    "KM = [KMeans(n_clusters=k).fit(X) for k in K]\n",
    "centroids = [k.cluster_centers_ for k in KM]\n",
    "\n",
    "D_k = [cdist(X, cent, 'euclidean') for cent in centroids]\n",
    "cIdx = [np.argmin(D, axis=1) for D in D_k]\n",
    "dist = [np.min(D, axis=1) for D in D_k]\n",
    "avgWithinSS = [sum(d) / X.shape[0] for d in dist]\n",
    "\n",
    "# Total within sum of square\n",
    "wcss = [sum(d**2) for d in dist]\n",
    "tss = sum(pdist(X)**2) / X.shape[0]\n",
    "bss = tss - wcss\n",
    "varExplained = bss / tss * 100\n",
    "\n",
    "# kIdx = 10 - 1\n",
    "\n",
    "kIdx = 2\n",
    "\n",
    "# Elbow curve\n",
    "# Set the size of the plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "ax1.plot(K, avgWithinSS, '*-')\n",
    "ax1.plot(K[kIdx], avgWithinSS[kIdx], marker=\"o\", markersize=12, markeredgewidth=2)\n",
    "ax1.set_xlabel(\"Number of clusters\")\n",
    "ax1.set_ylabel(\"Average within cluster sum of squares\")\n",
    "ax1.set_title(\"Elbow for KMeans clustering\")\n",
    "\n",
    "ax2.plot(K, varExplained, '*-')\n",
    "ax2.plot(K[kIdx], varExplained[kIdx], marker=\"o\", markersize=12, markeredgewidth=2)\n",
    "ax2.set_xlabel(\"Number of clusters\")\n",
    "ax2.set_ylabel(\"Percentage of variance explained\")\n",
    "ax2.set_title(\"Elbow for KMeans clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Silhouette Method\n",
    "> In 1986, Peter J. Rousseuw described the silhouette method, which aims to explain the consistency within cluster data. Silhouette value will range between -1 and 1. A high value indicates that items are well matched within the cluster and weakly matched to the neighboring cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from matplotlib import cm\n",
    "\n",
    "score = []\n",
    "for n_clusters in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    labels = kmeans.labels_\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    score.append(silhouette_score(X, labels, metric=\"euclidean\"))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "ax1.plot(score)\n",
    "ax1.set_xlabel(\"k\")\n",
    "ax1.set_ylabel(\"Silhouette Score\")\n",
    "ax1.set_title(\"Silhouette for K-means\")\n",
    "\n",
    "# Initialize the cluster with n_clusters value and a random generator\n",
    "model = KMeans(n_clusters=3, init=\"k-means++\", n_init=10, random_state=0)\n",
    "model.fit_predict(X)\n",
    "cluster_labels = np.unique(model.labels_)\n",
    "n_clusters = cluster_labels.shape[0]\n",
    "\n",
    "# Compute the silhouette scores for each sample\n",
    "silhoutte_vals = silhouette_samples(X, model.labels_)\n",
    "\n",
    "# Get spectral values for colormap\n",
    "cmap = cm.get_cmap(\"Spectral\")\n",
    "\n",
    "y_lower, y_upper = 0, 0\n",
    "yticks = []\n",
    "for i, c in enumerate(cluster_labels):\n",
    "    c_silhouette_vals = silhoutte_vals[cluster_labels]\n",
    "    c_silhouette_vals.sort()\n",
    "    y_upper += len(c_silhouette_vals)\n",
    "    color = cmap(float(i) / n_clusters)\n",
    "    ax2.barh(range(y_lower, y_upper), c_silhouette_vals, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "    yticks.append((y_lower + y_upper) / 2)\n",
    "    y_lower += len(c_silhouette_vals)\n",
    "\n",
    "silhouette_avg = np.mean(silhoutte_vals)\n",
    "\n",
    "ax2.set_yticks(yticks)\n",
    "ax2.set_yticklabels(cluster_labels+1)\n",
    "\n",
    "# The vertical line for average silhouette score of all the values\n",
    "ax2.axvline(x=silhouette_avg, linestyle='--')\n",
    "ax2.set_xlabel(\"Silhouette coefficient\")\n",
    "ax2.set_ylabel(\"Cluster\")\n",
    "ax2.set_title(\"Silhouette for K-means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hierarchical Clustering\n",
    "> Agglomerative clustering is a hierarchical cluster technique that builds nested clusters with a bottom-up approach where each data point starts in its own cluster and as we move up, the clusters are merged based on a distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Agglomerative Cluster\n",
    "model = AgglomerativeClustering(n_clusters=3)\n",
    "\n",
    "# Fit the model to the iris data set that we have previously imported\n",
    "model.fit(X)\n",
    "\n",
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris['pred_species'] = model.labels_\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(iris.species, iris.pred_species))\n",
    "print(\"Classification report:\", metrics.classification_report(iris.species, iris.pred_species))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hierarchical clusterings result arrangement can be better interpreted with dendrogram visualization. SciPy provides necessary functions for dendrogram visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import cophenet, dendrogram, linkage\n",
    "\n",
    "# Generate the linkage matrix\n",
    "Z = linkage(X, 'ward')\n",
    "c, coph_dists = cophenet(Z, pdist(X))\n",
    "\n",
    "# Calculate full dendrogram\n",
    "fig, ax = plt.subplots(figsize=(25, 10))\n",
    "\n",
    "ax.set_title(\"Agglomerative Hierarchical Clustering Dendrogram\")\n",
    "ax.set_xlabel(\"sample index\")\n",
    "ax.set_ylabel(\"distance\")\n",
    "dn = dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,      # rotates the x axis labels\n",
    "    leaf_font_size=8.,      # font size for the x axis labels\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "> Since we know that $k = 3$, we can cut the tree at a distance threshold of around ten to get exactly three distinct clusters.\n",
    "\n",
    "\n",
    "Principal Component Analysis (PCA)\n",
    "----------------------------------\n",
    "> Principal component analysis (PCA) is the most popular unsupervised linear transformation technique for dimensionality reduction. PCA finds the directions of maximum variance in high-dimensional data such that most of the information is retained, and projects it onto a smaller dimensional subspace.\n",
    "\n",
    "> The PCA approach can be summarized as follows:\n",
    "> - Standardize data.\n",
    "> - Use standardized data to generate a covariance matrix or correlation matrix.\n",
    "> - Perform eigen decomposition: compute eigenvectors that are the principal component, which will give the direction, and compute eigenvalues, which will give the magnitude.\n",
    "> - Sort the eigen pairs and select eigenvectors with the largest eigenvalues, which cumulatively captures information above a certain threshold (say 95%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Standardize data\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Create covariance matrix\n",
    "cov_mat = np.cov(X_std.T)\n",
    "\n",
    "print(f\"Covariance matrix\\n{cov_mat}\")\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "print(f\"Eigenvectors\\n{eig_vecs}\")\n",
    "print(f\"Eigenvalues\\n{eig_vals}\")\n",
    "\n",
    "# Sort eigenvalues in decreasing order\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))]\n",
    "\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot) * 100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(\"Cummulative Variance Explained\", cum_var_exp)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(range(4), var_exp, alpha=0.5, align=\"center\", label=\"Individual explained variance\")\n",
    "ax.step(range(4), cum_var_exp, where=\"mid\", label=\"Cumulative explained variance\")\n",
    "ax.set_xlabel(\"Principal components\")\n",
    "ax.set_ylabel(\"Explained variance ratio\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the preceding plot, we can see that the first three principal components explain 99% of the variance. Let's perform PCA using Scikit-learn and plot the first three eigenvectors.\n",
    "\n",
    "> source:\n",
    "> http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Import some data to play with\n",
    "Y = iris.target\n",
    "\n",
    "# To get a better understanding of interactionof the dimensions\n",
    "# Plot the first three PCA dimensions\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "X_reduced = PCA(n_components=3).fit_transform(iris.data)\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,\n",
    "           cmap=cm.Paired)\n",
    "ax.set_title(\"First three PCA directions\")\n",
    "ax.set_xlabel(\"1st eigenvector\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"2nd eigenvector\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"3rd eigenvector\")\n",
    "ax.w_zaxis.set_ticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "name": "python38364bitb790c22c1a684c1eac40ecab49941293"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
